{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe7982bb-84b9-41b4-bf18-c4aaac7b46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT00_2022.csv.gz\n",
      "   kept rows: 117,759\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT01_2022.csv.gz\n",
      "   kept rows: 110,043\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT02_2022.csv.gz\n",
      "   kept rows: 101,214\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT03_2022.csv.gz\n",
      "   kept rows: 93,902\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT04_2022.csv.gz\n",
      "   kept rows: 4,172\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_main_JT05_2022.csv.gz\n",
      "   kept rows: 4,119\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT00_2022.csv.gz\n",
      "   kept rows: 0\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT01_2022.csv.gz\n",
      "   kept rows: 0\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT02_2022.csv.gz\n",
      "   kept rows: 0\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT03_2022.csv.gz\n",
      "   kept rows: 0\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT04_2022.csv.gz\n",
      "   kept rows: 0\n",
      "→ Fetching: https://lehd.ces.census.gov/data/lodes/LODES8/md/od/md_od_aux_JT05_2022.csv.gz\n",
      "   kept rows: 0\n",
      "\n",
      "✅ Wrote CSV: outputs\\employment_commute_howard_2022.csv  (rows: 1,274)\n",
      "✅ Wrote Excel: outputs\\employment_commute_howard_2022.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import gzip\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "STATE_ABBR = \"md\"\n",
    "STATE_FIPS = \"24\"\n",
    "COUNTY_FIPS = \"027\"               # Howard County\n",
    "COUNTY_KEY = STATE_FIPS + COUNTY_FIPS  # \"24027\"\n",
    "\n",
    "YEAR = \"2022\"\n",
    "PARTS = [\"main\", \"aux\"]           # OD variants\n",
    "JOB_TYPES = [\"JT00\",\"JT01\",\"JT02\",\"JT03\",\"JT04\",\"JT05\"]  # All jobs + sectors\n",
    "OUTPUT_DIR = \"outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Official LODES8 base (note: you used LODES8; keeping that)\n",
    "BASE_URL = f\"https://lehd.ces.census.gov/data/lodes/LODES8/{STATE_ABBR}/od\"\n",
    "\n",
    "# Raw OD columns (no header in source files)\n",
    "OD_COLS = [\n",
    "    \"h_geocode\",\"w_geocode\",\"S000\",\n",
    "    \"SA01\",\"SA02\",\"SA03\",\n",
    "    \"SE01\",\"SE02\",\"SE03\",\n",
    "    \"SI01\",\"SI02\",\"SI03\"\n",
    "]\n",
    "\n",
    "def download_od(part: str, job_type: str, year: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Download one OD file, read with headers, and return a DataFrame.\n",
    "    Skips if 404 / missing.\n",
    "    \"\"\"\n",
    "    url = f\"{BASE_URL}/{STATE_ABBR}_od_{part}_{job_type}_{year}.csv.gz\"\n",
    "    print(f\"→ Fetching: {url}\")\n",
    "    r = requests.get(url, timeout=180)\n",
    "    if r.status_code == 404:\n",
    "        print(f\"   (missing) 404 Not Found — skipping\")\n",
    "        return pd.DataFrame(columns=OD_COLS)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # Decompress gz in-memory\n",
    "    buf = io.BytesIO(r.content)\n",
    "    with gzip.GzipFile(fileobj=buf, mode=\"rb\") as gz:\n",
    "        df = pd.read_csv(gz, header=None, names=OD_COLS, dtype=str)\n",
    "\n",
    "    # Cast numeric columns we need\n",
    "    df[\"S000\"] = pd.to_numeric(df[\"S000\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Annotate metadata\n",
    "    df[\"year\"] = int(year)\n",
    "    df[\"part\"] = part\n",
    "    df[\"job_type\"] = job_type\n",
    "    return df\n",
    "\n",
    "def filter_to_howard(df: pd.DataFrame, mode: str = \"either\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep rows linked to Howard County, MD (FIPS 24027).\n",
    "    mode:\n",
    "      - \"either\": keep if home OR work county == 24027\n",
    "      - \"home_only\": keep if home county == 24027\n",
    "      - \"work_only\": keep if work county == 24027\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    h_county = df[\"h_geocode\"].str.slice(0, 5)\n",
    "    w_county = df[\"w_geocode\"].str.slice(0, 5)\n",
    "\n",
    "    if mode == \"home_only\":\n",
    "        m = (h_county == COUNTY_KEY)\n",
    "    elif mode == \"work_only\":\n",
    "        m = (w_county == COUNTY_KEY)\n",
    "    else:\n",
    "        m = (h_county == COUNTY_KEY) | (w_county == COUNTY_KEY)\n",
    "\n",
    "    out = df.loc[m, [\"h_geocode\",\"w_geocode\",\"S000\",\"year\"]].copy()\n",
    "    # Helpful block-group IDs (12-digit GEOIDs)\n",
    "    out[\"home_bg\"] = out[\"h_geocode\"].str.slice(0, 12)\n",
    "    out[\"work_bg\"] = out[\"w_geocode\"].str.slice(0, 12)\n",
    "    out[\"home_county\"] = out[\"h_geocode\"].str.slice(0, 5)\n",
    "    out[\"work_county\"] = out[\"w_geocode\"].str.slice(0, 5)\n",
    "    return out\n",
    "\n",
    "def build_employment_commute(df_filtered: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the employment_commute table with required columns:\n",
    "    home_geo_id, work_geo_id, commuter_count, net_commute_flow, data_year, source\n",
    "    - Aggregates to block-group pairs\n",
    "    - Computes net_commute_flow for Howard County block groups:\n",
    "      inflow_to_bg - outflow_from_bg\n",
    "    \"\"\"\n",
    "    if df_filtered.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"home_geo_id\",\"work_geo_id\",\"commuter_count\",\n",
    "            \"net_commute_flow\",\"data_year\",\"source\"\n",
    "        ])\n",
    "\n",
    "    # 1) Aggregate to block-group pair (home_bg, work_bg)\n",
    "    pair = (\n",
    "        df_filtered\n",
    "        .groupby([\"home_bg\",\"work_bg\",\"year\"], as_index=False)[\"S000\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"home_bg\":\"home_geo_id\",\"work_bg\":\"work_geo_id\",\"S000\":\"commuter_count\"})\n",
    "    )\n",
    "\n",
    "    # 2) Compute inflow and outflow totals for EACH Howard County block group\n",
    "    # Inflow to a BG = sum of S000 where work_bg == that BG\n",
    "    inflow = (\n",
    "        df_filtered\n",
    "        .groupby(\"work_bg\", as_index=False)[\"S000\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"work_bg\":\"bg\",\"S000\":\"inflow\"})\n",
    "    )\n",
    "    # Outflow from a BG = sum of S000 where home_bg == that BG\n",
    "    outflow = (\n",
    "        df_filtered\n",
    "        .groupby(\"home_bg\", as_index=False)[\"S000\"]\n",
    "        .sum()\n",
    "        .rename(columns={\"home_bg\":\"bg\",\"S000\":\"outflow\"})\n",
    "    )\n",
    "    net = inflow.merge(outflow, on=\"bg\", how=\"outer\").fillna(0)\n",
    "    net[\"net_commute_flow_bg\"] = net[\"inflow\"] - net[\"outflow\"]\n",
    "\n",
    "    # Keep net only for Howard County BGs (first 5 digits == 24027)\n",
    "    net_howard = net[net[\"bg\"].str.slice(0,5) == COUNTY_KEY][[\"bg\",\"net_commute_flow_bg\"]].copy()\n",
    "\n",
    "    # 3) Attach a per-row net_commute_flow:\n",
    "    # - If work_geo_id is in Howard, use net for work_geo_id\n",
    "    # - Else if home_geo_id is in Howard, use net for home_geo_id\n",
    "    pair = pair.merge(net_howard.rename(columns={\"bg\":\"work_geo_id\",\"net_commute_flow_bg\":\"_net_work\"}),\n",
    "                      on=\"work_geo_id\", how=\"left\")\n",
    "    pair = pair.merge(net_howard.rename(columns={\"bg\":\"home_geo_id\",\"net_commute_flow_bg\":\"_net_home\"}),\n",
    "                      on=\"home_geo_id\", how=\"left\")\n",
    "\n",
    "    def choose_net(row):\n",
    "        if isinstance(row[\"_net_work\"], (int, float)) and pd.notna(row[\"_net_work\"]):\n",
    "            return int(row[\"_net_work\"])\n",
    "        if isinstance(row[\"_net_home\"], (int, float)) and pd.notna(row[\"_net_home\"]):\n",
    "            return int(row[\"_net_home\"])\n",
    "        return 0  # neither side is a Howard BG\n",
    "\n",
    "    pair[\"net_commute_flow\"] = pair.apply(choose_net, axis=1)\n",
    "    pair = pair.drop(columns=[\"_net_work\",\"_net_home\"])\n",
    "\n",
    "    # 4) Add metadata columns\n",
    "    pair[\"data_year\"] = pair[\"year\"].astype(int)\n",
    "    pair[\"source\"] = \"LEHD LODES8 OD\"\n",
    "    pair = pair.drop(columns=[\"year\"])\n",
    "\n",
    "    # Order columns exactly as requested\n",
    "    pair = pair[[\n",
    "        \"home_geo_id\",\"work_geo_id\",\"commuter_count\",\n",
    "        \"net_commute_flow\",\"data_year\",\"source\"\n",
    "    ]]\n",
    "\n",
    "    return pair\n",
    "\n",
    "def main():\n",
    "    combined = []\n",
    "    for part in PARTS:\n",
    "        for jt in JOB_TYPES:\n",
    "            try:\n",
    "                df = download_od(part, jt, YEAR)\n",
    "                if df.empty:\n",
    "                    continue\n",
    "                dff = filter_to_howard(df, mode=\"either\")\n",
    "                if not dff.empty:\n",
    "                    combined.append(dff)\n",
    "                print(f\"   kept rows: {len(dff):,}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ERROR on {part} {jt}: {e}\")\n",
    "\n",
    "    if not combined:\n",
    "        print(\"No data collected. Check availability/year.\")\n",
    "        return\n",
    "\n",
    "    df_filtered = pd.concat(combined, ignore_index=True)\n",
    "\n",
    "    # Build schema-aligned employment_commute table\n",
    "    commute = build_employment_commute(df_filtered)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"employment_commute_howard_{YEAR}.csv\")\n",
    "    commute.to_csv(csv_path, index=False)\n",
    "    print(f\"\\n✅ Wrote CSV: {csv_path}  (rows: {len(commute):,})\")\n",
    "\n",
    "    # Save Excel (single sheet: employment_commute)\n",
    "    xlsx_path = os.path.join(OUTPUT_DIR, f\"employment_commute_howard_{YEAR}.xlsx\")\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"xlsxwriter\") as writer:\n",
    "        commute.to_excel(writer, index=False, sheet_name=\"employment_commute\")\n",
    "    print(f\"✅ Wrote Excel: {xlsx_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a5030-1b58-4647-8fc0-55309da1da99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
